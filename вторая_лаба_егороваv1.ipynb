{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ygarkakoito/neuron/blob/main/%D0%B2%D1%82%D0%BE%D1%80%D0%B0%D1%8F_%D0%BB%D0%B0%D0%B1%D0%B0_%D0%B5%D0%B3%D0%BE%D1%80%D0%BE%D0%B2%D0%B0v1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "PqC4R7SGseKa"
      },
      "outputs": [],
      "source": [
        "import torch\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0J2RM8f5wP33"
      },
      "source": [
        "## 2.1 Создание нейронов и полносвязных слоев"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_2ArJn_nsdZC"
      },
      "source": [
        "2.1.1. Используя операции над матрицами и векторами из библиотеки `torch`, реализовать нейрон с заданными весами `weights` и `bias`. Прогнать вектор `inputs` через нейрон и вывести результат."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "f4agkY9WqPwe"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "class Neuron:\n",
        "    def __init__(self, weights, bias):\n",
        "        # Инициализация весов и смещения\n",
        "        self.weights = weights\n",
        "        self.bias = bias\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # Вычисление выхода нейрона (взвешенная сумма входов + смещение)\n",
        "        output = torch.dot(self.weights, inputs) + self.bias\n",
        "        return output\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "HJRkSkHHsb7u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "247ec878-73f6-47a3-9e6a-a20a9f58c900"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(4.8400)\n"
          ]
        }
      ],
      "source": [
        "inputs = torch.tensor([1.0, 2.0, 3.0, 4.0])\n",
        "# Заданные веса\n",
        "weights = torch.tensor([-0.2, 0.3, -0.5, 0.7])\n",
        "#смещение\n",
        "bias = 3.14\n",
        "#нейрон с заданными параметрами\n",
        "neuron = Neuron(weights, bias)\n",
        "#входные данные через нейрон\n",
        "output = neuron.forward(inputs)\n",
        "print( output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1qJvnwiyty37"
      },
      "source": [
        "2.1.2 Используя операции над матрицами и векторами из библиотеки `torch`, реализовать полносвязный слой с заданными весами `weights` и `biases`. Прогнать вектор `inputs` через слой и вывести результат."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "fVWF3a9vtx90"
      },
      "outputs": [],
      "source": [
        "class Linear:\n",
        "    def __init__(self, weights, biases):\n",
        "        # Инициализация весов и смещений\n",
        "        self.weights = weights\n",
        "        self.biases = biases\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # выход слоя после применения взвешенной суммы входныщ данных и добавления смещений.\n",
        "        output = torch.matmul(inputs, self.weights) + self.biases #matmul - матричное умножение между двумя тензорами\n",
        "        return output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Fo-JFnHPuFCS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9435250-30cd-422b-bf17-23c6b0f86799"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 4.8400,  0.1700, 10.3900])\n"
          ]
        }
      ],
      "source": [
        "inputs = torch.tensor([1.0, 2.0, 3.0, 4.0])\n",
        "#  веса (транспонированные, чтобы соответствовать матричному умножению) и смещения\n",
        "weights = torch.tensor([[-0.2, 0.3, -0.5, 0.7],\n",
        "                        [0.5, -0.91, 0.26, -0.5],\n",
        "                        [-0.26, -0.27, 0.17, 0.87]]).T    #.T -  в библиотеке PyTorch транспонирование матрицы.\n",
        "biases = torch.tensor([3.14, 2.71, 7.2])\n",
        "\n",
        "# Создаем полносвязный слой с заданными параметрами\n",
        "layer = Linear(weights, biases)\n",
        "\n",
        "# Прогоняем входные данные через слой\n",
        "output = layer.forward(inputs)\n",
        "print( output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQtsJzcxuyGd"
      },
      "source": [
        "2.1.3 Реализовать полносвязный слой из __2.1.2__ таким образом, чтобы он мог принимать на вход матрицу (батч) с данными. Продемонстрировать работу.\n",
        "Результатом прогона сквозь слой должна быть матрица размера `batch_size` x `n_neurons`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Z8IizmtsuhO1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a475dff1-5006-4eed-c584-5212729f76f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-2.6427, -1.2913,  9.9646],\n",
            "        [ 2.7228, -2.8230,  7.9035],\n",
            "        [-9.4826,  1.6502,  3.3985]])\n"
          ]
        }
      ],
      "source": [
        "class LinearLayer:\n",
        "    def __init__(self, input_size, output_size):\n",
        "        # Инициализация весов и смещений случайными значениями\n",
        "        self.weights = torch.randn(input_size, output_size)\n",
        "        self.biases = torch.zeros(1, output_size)  #torch.zeros - создает тензор заполненный нулями.\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # Вычисление выхода слоя\n",
        "        output = torch.matmul(inputs, self.weights) + self.biases\n",
        "        return output\n",
        "\n",
        "inputs = torch.tensor([[1, 2, 3, 2.5],\n",
        "                       [2, 5, -1, 2],\n",
        "                       [-1.5, 2.7, 3.3, -0.8]])\n",
        "\n",
        "# Создаем полносвязный слой с заданными размерами\n",
        "batch_size = inputs.shape[0]   # inputs.shape[1] -  размерность Матрицы\n",
        "input_size = inputs.shape[1]   # число признаков\n",
        "output_size = 3               # Число нейронов\n",
        "# Создаем полносвязный слой с заданными размерами\n",
        "layer = LinearLayer(input_size, output_size)\n",
        "\n",
        "# Прогоняем входные данные через слой\n",
        "output = layer.forward(inputs)\n",
        "print(output)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQ2OxH4_vBLu"
      },
      "source": [
        "2.1.4 Используя операции над матрицами и векторами из библиотеки `torch`, реализовать полносвязный слой из `n_neurons` нейронов с `n_features` весами у каждого нейрона (инициализируются из стандартного нормального распределения). Прогнать вектор `inputs` через слой и вывести результат. Результатом прогона сквозь слой должна быть матрица размера `batch_size` x `n_neurons`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "IOv52EdovASs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a5bc256-c1fe-4d0c-d5f1-36c789ce8fc5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 1.6763, -2.3353, -1.3512,  0.4950, -4.2232, -7.9134, -0.1491, -1.3697,\n",
            "         -0.0851],\n",
            "        [ 0.6528,  2.4480, -6.9328, -3.5239,  5.5778,  1.1055,  0.6546,  6.2809,\n",
            "         -1.3707],\n",
            "        [ 9.2531,  0.1709, -2.2112, -0.1650, -5.9738, -4.8129,  5.2242,  0.3557,\n",
            "          0.1102]])\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "\n",
        "class Linear:\n",
        "    def __init__(self, n_features, n_neurons):\n",
        "        # Инициализация весов из стандартного нормального распределения и смещений нулями\n",
        "        self.weights = torch.randn(n_features, n_neurons)\n",
        "        self.biases = torch.zeros(1, n_neurons)#создается тензор размером (1, n_neurons), где 1 количество строк и n_neurons - количество нейронов в слое. Таким образом, для каждого нейрона у нас есть одно смещение.\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # Вычисление выхода слоя\n",
        "        output = torch.matmul(inputs, self.weights) + self.biases\n",
        "        return output\n",
        "\n",
        "inputs = torch.tensor([[1, 2, 3, 2.5],\n",
        "                       [2, 5, -1, 2],\n",
        "                       [-1.5, 2.7, 3.3, -0.8]])\n",
        "\n",
        "# Получаем размеры входных данных (batch_size)\n",
        "batch_size, n_features = inputs.shape\n",
        "n_neurons = random.randint(1, 10)\n",
        "# Создаем полносвязный слой с заданными параметрами\n",
        "layer = Linear(n_features, n_neurons)\n",
        "# Прогоняем входные данные через слой\n",
        "output = layer.forward(inputs)\n",
        "print(output)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPG4UqL4wajI"
      },
      "source": [
        "2.1.5 Используя решение из __2.1.4__, создать 2 полносвязных слоя и пропустить матрицу `inputs` последовательно через эти два слоя. Количество нейронов в первом слое выбрать произвольно, количество нейронов во втором слое выбрать так, чтобы результатом прогона являлась матрица (3x7)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "RjjQIQlTxJE6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60bdcc7d-9e01-42c7-ba0a-00ef919a328f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ -1.1048,  -4.0769,  -4.7125,  -1.0234,  -0.5207,   1.1904,   0.5682],\n",
            "        [ -6.3920, -10.0574,  -2.2585,   5.5514,   3.8663,  -1.6230,   3.4153],\n",
            "        [  2.5446,  -2.8737,  -4.8097,   4.3865,  -1.1289,   8.7595,   8.9266]]) tensor([[ -2.2888,  -8.1606,  -1.5393,   4.4760,   3.5239,  -3.8310,  10.7168],\n",
            "        [ -6.6715,  -0.6912, -19.8736, -24.0452,   5.5523, -13.7955,  43.2604],\n",
            "        [ -2.0772, -22.9993,   3.1319,   0.5813,  10.6662,  10.5072,   6.8514]])\n"
          ]
        }
      ],
      "source": [
        "class Linear:\n",
        "    def __init__(self, input_size, output_size):\n",
        "        # Инициализация весов из стандартного нормального распределения и смещений нулями\n",
        "        self.weights = torch.randn(input_size, output_size)\n",
        "        self.biases = torch.zeros(1, output_size)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # Вычисление выхода слоя\n",
        "        output = torch.matmul(inputs, self.weights) + self.biases\n",
        "        return output\n",
        "\n",
        "inputs = torch.tensor([[1, 2, 3, 2.5],\n",
        "                       [2, 5, -1, 2],\n",
        "                       [-1.5, 2.7, 3.3, -0.8]])\n",
        "\n",
        "# Получаем размеры данных (batch_size)\n",
        "batch_size, input_size = inputs.shape\n",
        "# Задаем произвольное количество нейронов в первом слое\n",
        "n_neurons_1 = random.randint(1, 10)\n",
        "n_neurons_2 = 7\n",
        "\n",
        "# Создаем два полносвязных слоя\n",
        "layer1 = Linear(input_size, n_neurons_1)\n",
        "layer2 = Linear(n_neurons_1, n_neurons_2)\n",
        "\n",
        "# Прогоняем входные данные через слоя\n",
        "output1 = layer1.forward(inputs)\n",
        "output2 = layer2.forward(output1)\n",
        "\n",
        "print(output1,output2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRVH_2K7xTBC"
      },
      "source": [
        "## 2.2 Создание функций активации"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B9kngE6Fxs9D"
      },
      "source": [
        "2.2.1 Используя операции над матрицами и векторами из библиотеки `torch`, реализовать функцию активации ReLU:\n",
        "\n",
        "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/f4353f4e3e484130504049599d2e7b040793e1eb)\n",
        "\n",
        "Создать матрицу размера (4,3), заполненную числами из стандартного нормального распределения, и проверить работоспособность функции активации."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "jZLvMRByxSTC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d41f7e5-9649-4db8-b261-aa35f1ad6e92"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Исходная матрица: tensor([[ 1.6418, -0.5919, -0.7107],\n",
            "        [ 0.8002,  0.2380, -0.2935],\n",
            "        [-0.9242,  0.2684,  0.5580],\n",
            "        [ 0.9941,  1.1855,  0.2210]])\n",
            "Результат после применения ReLU: tensor([[1.6418, 0.0000, 0.0000],\n",
            "        [0.8002, 0.2380, 0.0000],\n",
            "        [0.0000, 0.2684, 0.5580],\n",
            "        [0.9941, 1.1855, 0.2210]])\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "class ReLU:\n",
        "    def forward(self, inputs):\n",
        "        result = torch.where(inputs > 0, inputs, torch.tensor(0.0)) # ReLU заменяет отрицательные значения на нули, оставляя положительные значения без изменений. (where просто условная оперция типа того)\n",
        "        return result\n",
        "\n",
        "matrix = torch.randn(4, 3)\n",
        "relu = ReLU()\n",
        "output = relu.forward(matrix)\n",
        "\n",
        "print(\"Исходная матрица:\",matrix)\n",
        "print(\"Результат после применения ReLU:\",output)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "puExCWiKyTtb"
      },
      "source": [
        "2.2.2 Используя операции над матрицами и векторами из библиотеки `torch`, реализовать функцию активации softmax:\n",
        "\n",
        "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/6d7500d980c313da83e4117da701bf7c8f1982f5)\n",
        "\n",
        "Создать матрицу размера (4,3), заполненную числами из стандартного нормального распределения, и проверить работоспособность функции активации. Строки матрицы трактовать как выходы линейного слоя некоторого классификатора для 4 различных примеров."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "fXNcFlqqyKHl"
      },
      "outputs": [],
      "source": [
        "class Softmax:\n",
        "  def forward(self, x):\n",
        "    res = torch.exp(x)/(torch.exp(x).sum())\n",
        "    return res\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nn = torch.nn.Softmax(dim=1)\n",
        "nn.forward(inputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FWZMyjVD1g45",
        "outputId": "2647680d-df2f-4529-d05a-aac13e1617e6"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.0641, 0.1744, 0.4740, 0.2875],\n",
              "        [0.0452, 0.9074, 0.0022, 0.0452],\n",
              "        [0.0052, 0.3488, 0.6355, 0.0105]])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = torch.tensor([[1, 2, 3, 2.5],\n",
        "                       [2, 5, -1, 2],\n",
        "                       [-1.5, 2.7, 3.3, -0.8]])\n",
        "ss=Softmax()\n",
        "ss.forward(inputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CzoPTMcm02g4",
        "outputId": "b8afe128-fcbd-489e-9bdf-84fbba08de0d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.0109, 0.0297, 0.0808, 0.0490],\n",
              "        [0.0297, 0.5970, 0.0015, 0.0297],\n",
              "        [0.0009, 0.0599, 0.1091, 0.0018]])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vxVK2TYez_Ye"
      },
      "source": [
        "2.2.3 Используя операции над матрицами и векторами из библиотеки `torch`, реализовать функцию активации ELU:\n",
        "\n",
        "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/eb23becd37c3602c4838e53f532163279192e4fd)\n",
        "\n",
        "Создать матрицу размера (4,3), заполненную числами из стандартного нормального распределения, и проверить работоспособность функции активации."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "NzMz7HDLySxK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f915c934-d106-44d5-822e-25e419697c42"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Исходная матрица: tensor([[ 0.8634,  0.9338, -0.5790],\n",
            "        [-0.1234,  0.6972, -0.1750],\n",
            "        [ 0.4908, -0.8319, -0.9447],\n",
            "        [ 1.1324,  0.4761,  0.0046]])\n",
            "Результат после применения ELU: tensor([[ 0.8634,  0.9338, -0.4396],\n",
            "        [-0.1161,  0.6972, -0.1606],\n",
            "        [ 0.4908, -0.5648, -0.6112],\n",
            "        [ 1.1324,  0.4761,  0.0046]])\n"
          ]
        }
      ],
      "source": [
        "class ELU:\n",
        "    def __init__(self, alpha):\n",
        "        self.alpha = alpha\n",
        "\n",
        "    def forward(self, inputs):\n",
        "\n",
        "        result = torch.where(inputs >= 0, inputs, self.alpha * (torch.exp(inputs) - 1)) #заменяем отрицательные значения в inputs на значения, вычисленные с помощью функции активации ELU (a * (e^x - 1)), где a - параметр alpha. Если значение положительное или ноль, то оно остается без изменений.\n",
        "        return result\n",
        "\n",
        "matrix = torch.randn(4, 3)\n",
        "elu = ELU(alpha=1.0)\n",
        "output = elu.forward(matrix)\n",
        "print(\"Исходная матрица:\",matrix)\n",
        "print(\"Результат после применения ELU:\",output)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0peh8r-20Pof"
      },
      "source": [
        "## 2.3 Создание функции потерь"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EY-k3eEs0f7f"
      },
      "source": [
        "2.3.1 Используя операции над матрицами и векторами из библиотеки `torch`, реализовать функцию потерь MSE:\n",
        "\n",
        "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/e258221518869aa1c6561bb75b99476c4734108e)\n",
        "\n",
        "Создать полносвязный слой с 1 нейроном, прогнать через него батч `inputs` и посчитать значение MSE, трактуя вектор `y` как вектор правильных ответов."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "f9-wdj5Tz-br",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3d49e58-fa2c-4c1f-ae9f-c1cb765eb54a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Предсказанные значения: tensor([[-0.0984],\n",
            "        [ 4.0748],\n",
            "        [ 4.0577]]) Истинные значения: tensor([[2],\n",
            "        [3],\n",
            "        [4]]) Значение MSE: tensor(1.8539)\n"
          ]
        }
      ],
      "source": [
        "class Linear:\n",
        "    def __init__(self, input_size, output_size):\n",
        "        self.weights = torch.randn(input_size, output_size)\n",
        "        self.biases = torch.zeros(1, output_size)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        output = torch.matmul(inputs, self.weights) + self.biases\n",
        "        return output\n",
        "\n",
        "class MSELoss:\n",
        "    def forward(self, y_pred, y_true):\n",
        "        mse = torch.mean((y_pred - y_true) ** 2)\n",
        "        return mse\n",
        "\n",
        "inputs = torch.tensor([[1, 2, 3, 2.5],\n",
        "                       [2, 5, -1, 2],\n",
        "                       [-1.5, 2.7, 3.3, -0.8]])\n",
        "\n",
        "# Истинные значения\n",
        "y_true = torch.tensor([2, 3, 4]).reshape(-1, 1)  # вектор в матрицу 3, 1\n",
        "\n",
        "# Создаем полносвязный слой с 1 нейроном\n",
        "input_size = inputs.shape[1]  #число признаков\n",
        "output_size = 1  # Один нейрон для регрессии\n",
        "layer = Linear(input_size, output_size)\n",
        "y_pred = layer.forward(inputs)\n",
        "mse_loss = MSELoss()\n",
        "loss = mse_loss.forward(y_pred, y_true)\n",
        "\n",
        "print(\"Предсказанные значения:\",y_pred,\"Истинные значения:\",y_true,\"Значение MSE:\",loss)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NAyuDU9F1Vuz"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uaR7rILd1eWR"
      },
      "source": [
        "2.3.2 Используя операции над матрицами и векторами из библиотеки `torch`, реализовать функцию потерь Categorical Cross-Entropy:\n",
        "\n",
        "<img src=\"https://i.ibb.co/93gy1dN/Screenshot-9.png\" width=\"200\">\n",
        "\n",
        "Создать полносвязный слой с 3 нейронами и прогнать через него батч `inputs`. Полученный результат пропустить через функцию активации softmax. Посчитать значение CCE, трактуя вектор `y` как вектор правильных ответов."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "hQl8pJsT3HcF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6aa937fd-ade2-4fb4-c807-7e9c44456506"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Предсказанные значения (Softmax):\n",
            "tensor([[2.0851e-06, 9.7232e-01, 2.7673e-02],\n",
            "        [7.4069e-03, 4.2856e-03, 9.8831e-01],\n",
            "        [3.7651e-06, 9.9999e-01, 1.1010e-05]])\n",
            "Истинные значения:\n",
            "tensor([[1, 0, 0],\n",
            "        [0, 1, 0],\n",
            "        [0, 0, 1]])\n",
            "Значение Categorical Cross-Entropy Loss:\n",
            "tensor(29.9499)\n"
          ]
        }
      ],
      "source": [
        "class CategoricalCrossentropyLoss:\n",
        "    def forward(self, y_pred, y_true):\n",
        "        cce = -torch.sum(y_true * torch.log(y_pred))\n",
        "        return cce\n",
        "\n",
        "class Softmax:\n",
        "    def forward(self, x):\n",
        "        # Применение функции активации Softmax\n",
        "        result = torch.exp(x - torch.max(x, dim=1, keepdim=True).values)\n",
        "        result /= torch.sum(result, dim=1, keepdim=True)  #keepdim=True  -контролирует, будут ли сохранены или уменьшены размерности тензора после выполнения операции (то есть размерность будет dim=1).\n",
        "        return result\n",
        "\n",
        "inputs = torch.tensor([[1, 2, 3, 2.5],\n",
        "                       [2, 5, -1, 2],\n",
        "                       [-1.5, 2.7, 3.3, -0.8]])\n",
        "\n",
        "y_true = torch.tensor([[1, 0, 0],\n",
        "                       [0, 1, 0],\n",
        "                       [0, 0, 1]])\n",
        "\n",
        "input_size = inputs.shape[1]\n",
        "output_size = 3\n",
        "layer = Linear(input_size, output_size)\n",
        "y_pred = layer.forward(inputs)\n",
        "softmax = Softmax()\n",
        "y_pred_softmax = softmax.forward(y_pred)\n",
        "cce_loss = CategoricalCrossentropyLoss()\n",
        "loss = cce_loss.forward(y_pred_softmax, y_true)\n",
        "\n",
        "print(\"Предсказанные значения (Softmax):\")\n",
        "print(y_pred_softmax)\n",
        "print(\"Истинные значения:\")\n",
        "print(y_true)\n",
        "print(\"Значение Categorical Cross-Entropy Loss:\")\n",
        "print(loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s7Qoupfo1ZGJ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fA6dbanf44_4"
      },
      "source": [
        "2.3.3 Модифицировать 2.3.1, добавив L2-регуляризацию.\n",
        "\n",
        "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/d92ca2429275bfdc0474523babbafe014ca8b580)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ADsZxD-h4_Os"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MSELossL2:\n",
        "    def __init__(self, lambda_):\n",
        "        self.lambda_ = lambda_\n",
        "\n",
        "    def data_loss(self, y_pred, y_true):\n",
        "        # Вычисление потери данных (MSE)\n",
        "        mse = torch.mean((y_pred - y_true) ** 2)\n",
        "        return mse\n",
        "\n",
        "    def reg_loss(self, layer):\n",
        "        # Вычисление регуляризационной потери L2\n",
        "        l2_loss = torch.sum(layer.weights ** 2) * self.lambda_  # L2-регуляризация\n",
        "        return l2_loss\n",
        "\n",
        "    def forward(self, y_pred, y_true, layer):\n",
        "        # Общая потеря как сумма потери данных и регуляризационной потери\n",
        "        total_loss = self.data_loss(y_pred, y_true) + self.reg_loss(layer)\n",
        "        return total_loss\n",
        "\n",
        "\n",
        "input_size = inputs.shape[1]\n",
        "output_size = 1\n",
        "layer = Linear(input_size, output_size)\n",
        "y_pred = layer.forward(inputs)\n",
        "\n",
        "# MSELossL2 с параметром lambda_=0.01\n",
        "lambda_ = 0.01\n",
        "mse_loss_l2 = MSELossL2(lambda_)\n",
        "\n",
        "#общая потеря с учетом L2-регуляризации\n",
        "total_loss = mse_loss_l2.forward(y_pred, y_true, layer)\n",
        "\n",
        "print(\"Предсказанные значения:\", y_pred)\n",
        "print(\"Истинные значения:\", y_true)\n",
        "print(\"Значение MSELoss с L2-регуляризацией:\", total_loss)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EJnYgUoIXDBZ",
        "outputId": "6f83456f-4cc2-43cc-e3aa-e6e2a070dc50"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Предсказанные значения: tensor([[-6.4279],\n",
            "        [-6.5120],\n",
            "        [-5.7902]])\n",
            "Истинные значения: tensor([[1, 0, 0],\n",
            "        [0, 1, 0],\n",
            "        [0, 0, 1]])\n",
            "Значение MSELoss с L2-регуляризацией: tensor(43.6073)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w049ZSdR6qQi"
      },
      "source": [
        "## 2.4 Обратное распространение ошибки"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pBtCfSME9W7Q"
      },
      "source": [
        "2.4.1 Используя один нейрон и SGD (1 пример за шаг), решите задачу регрессии"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "4xmI-QJ66WAF"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import make_regression\n",
        "X, y, coef = make_regression(n_features=4, n_informative=4, coef=True, bias=0.5)  #n_features: Это параметр, который указывает количество признаков (факторов), которые будут участвовать в созданных данных\n",
        "#n_informative: Это параметр, который указывает количество информативных признаков, которые действительно влияют на целевую величину, которую мы пытаемся предсказать\n",
        "X = torch.from_numpy(X).to(torch.float32) #преобразует массив X в тензор\n",
        "y = torch.from_numpy(y).to(torch.float32)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "coef"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GELIOETT_WUa",
        "outputId": "c2f15eb6-6a54-4ba5-9acb-225f504464f8"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([65.80849942, 69.58911489, 48.32251677, 15.31132061])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KpPSPYSpD9Ey"
      },
      "source": [
        "[Граф вычислений для этой задачи](https://i.ibb.co/2dhDxZx/photo-2021-02-15-17-18-04.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fc1sXtGd_J-y"
      },
      "source": [
        "2.4.1.1 Реализуйте класс `SquaredLoss`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "llFigkqd_JRU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e4ca363-a698-44ad-8ad9-1259d6c01a32"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.0833)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "class SquaredLoss:\n",
        "  def forward(self, y_pred, y_true):\n",
        "    #E = torch.square(y_pred - y_true).mean()\n",
        "    E =((y_pred-y_true)**2).mean()\n",
        "    return E # <реализовать логику MSE>\n",
        "\n",
        "  def backward(self, y_pred, y_true):\n",
        "    self.dinput = 2*(y_pred-y_true) #-(2/y_pred.size()[0])*[(y_pred-y_true)] # df/dc #dinput-градиент функции потерь\n",
        "\n",
        "y_pred = torch.tensor([2,3,4])\n",
        "y_t = torch.tensor([2,3,4.5])\n",
        "sql=SquaredLoss()\n",
        "sql.forward(y_pred,y_t)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GY7ForfM97UQ"
      },
      "source": [
        "2.4.1.2. Модифицируйте класс `Neuron` из __2.1.1__:\n",
        "\n",
        "  1) Сделайте так, чтобы веса нейрона инициализировались из стандартного нормального распределения\n",
        "\n",
        "  2) Реализуйте расчет градиента относительно весов `weights` и `bias`"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "U4100xrlLAoP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "L0KqxPJU9kAN"
      },
      "outputs": [],
      "source": [
        "class Neuron:\n",
        "  def __init__(self, n_inputs):\n",
        "    self.w=torch.randn(n_inputs)\n",
        "    self.b=torch.randn(1)\n",
        "    pass\n",
        "\n",
        "  def forward(self, inputs):\n",
        "    self.inputs=inputs\n",
        "    return torch.dot(inputs,self.w)+self.b #Вычисляет взвешенную сумму входных данных и весов нейрона, затем добавляет смещение\n",
        "    #torch.dot(inputs, self.w): вычисляет скалярное произведение между вектором входных данных inputs и вектором весов нейрона self.w\n",
        "\n",
        "  def backward(self, dvalue):\n",
        "    # dvalue - значение производной, которое приходит нейрону от следующего слоя сети\n",
        "    # в данном случае это будет значение df/dc (созданное методом backwards у объекта MSELoss)\n",
        "    self.dweights = self.inputs*dvalue  #dweights- градиент (производная) потери по отношению к весам (weights) нейрона\n",
        "    self.dinput =  self.w*dvalue  #dinput-градиент функции потерь\n",
        "    self.dbias =dvalue\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKcO4zOLACxM"
      },
      "source": [
        "2.4.1.3 Допишите цикл для настройки весов нейрона\n",
        "\n",
        "[SGD](https://ru.wikipedia.org/wiki/%D0%A1%D1%82%D0%BE%D1%85%D0%B0%D1%81%D1%82%D0%B8%D1%87%D0%B5%D1%81%D0%BA%D0%B8%D0%B9_%D0%B3%D1%80%D0%B0%D0%B4%D0%B8%D0%B5%D0%BD%D1%82%D0%BD%D1%8B%D0%B9_%D1%81%D0%BF%D1%83%D1%81%D0%BA)\n",
        "\n",
        "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/dda3670f8a8996a0d3bf80856bb4a166cc8db6d4)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_inputs = X.size()[1] #Определяет количество входных признаков (нейронов) на основе размерности матрицы X.\n",
        "lr = 0.1 #  скорость обучения\n",
        "n_epoch = 100 #  количество эпох\n",
        "\n",
        "neuron = Neuron(n_inputs)\n",
        "loss = SquaredLoss()"
      ],
      "metadata": {
        "id": "eZFPTk9qOa62"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "_g_FvwvmALJd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46f13247-026d-4085-8846-3e6d7f4aee3b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([-0.7654, -1.0248, -0.6929,  0.5084])\n",
            "tensor([65.8071, 69.5808, 48.3187, 15.3064])\n",
            "tensor([65.8085, 69.5891, 48.3225, 15.3113])\n",
            "tensor([65.8085, 69.5891, 48.3225, 15.3113])\n",
            "tensor([65.8085, 69.5891, 48.3225, 15.3113])\n",
            "tensor([65.8085, 69.5891, 48.3225, 15.3113])\n",
            "tensor([65.8085, 69.5891, 48.3225, 15.3113])\n",
            "tensor([65.8085, 69.5891, 48.3225, 15.3113])\n",
            "tensor([65.8085, 69.5891, 48.3225, 15.3113])\n",
            "tensor([65.8085, 69.5891, 48.3225, 15.3113])\n",
            "tensor([65.8085, 69.5891, 48.3225, 15.3113])\n",
            "tensor([65.8085, 69.5891, 48.3225, 15.3113])\n",
            "tensor([65.8085, 69.5891, 48.3225, 15.3113])\n",
            "tensor([65.8085, 69.5891, 48.3225, 15.3113])\n",
            "tensor([65.8085, 69.5891, 48.3225, 15.3113])\n",
            "tensor([65.8085, 69.5891, 48.3225, 15.3113])\n",
            "tensor([65.8085, 69.5891, 48.3225, 15.3113])\n",
            "tensor([65.8085, 69.5891, 48.3225, 15.3113])\n",
            "tensor([65.8085, 69.5891, 48.3225, 15.3113])\n",
            "tensor([65.8085, 69.5891, 48.3225, 15.3113])\n",
            "tensor([65.8085, 69.5891, 48.3225, 15.3113])\n",
            "tensor([65.8085, 69.5891, 48.3225, 15.3113])\n",
            "tensor([65.8085, 69.5891, 48.3225, 15.3113])\n",
            "tensor([65.8085, 69.5891, 48.3225, 15.3113])\n",
            "tensor([65.8085, 69.5891, 48.3225, 15.3113])\n",
            "tensor([65.8085, 69.5891, 48.3225, 15.3113])\n",
            "tensor([65.8085, 69.5891, 48.3225, 15.3113])\n",
            "tensor([65.8085, 69.5891, 48.3225, 15.3113])\n",
            "tensor([65.8085, 69.5891, 48.3225, 15.3113])\n",
            "tensor([65.8085, 69.5891, 48.3225, 15.3113])\n",
            "tensor([65.8085, 69.5891, 48.3225, 15.3113])\n",
            "tensor([65.8085, 69.5891, 48.3225, 15.3113])\n",
            "tensor([65.8085, 69.5891, 48.3225, 15.3113])\n",
            "tensor([65.8085, 69.5891, 48.3225, 15.3113])\n",
            "tensor([65.8085, 69.5891, 48.3225, 15.3113])\n",
            "tensor([65.8085, 69.5891, 48.3225, 15.3113])\n",
            "tensor([65.8085, 69.5891, 48.3225, 15.3113])\n",
            "tensor([65.8085, 69.5891, 48.3225, 15.3113])\n",
            "tensor([65.8085, 69.5891, 48.3225, 15.3113])\n",
            "tensor([65.8085, 69.5891, 48.3225, 15.3113])\n",
            "tensor([65.8085, 69.5891, 48.3225, 15.3113])\n",
            "tensor([65.8085, 69.5891, 48.3225, 15.3113])\n",
            "tensor([65.8085, 69.5891, 48.3225, 15.3113])\n",
            "tensor([65.8085, 69.5891, 48.3225, 15.3113])\n",
            "tensor([65.8085, 69.5891, 48.3225, 15.3113])\n",
            "tensor([65.8085, 69.5891, 48.3225, 15.3113])\n",
            "tensor([65.8085, 69.5891, 48.3225, 15.3113])\n",
            "tensor([65.8085, 69.5891, 48.3225, 15.3113])\n",
            "tensor([65.8085, 69.5891, 48.3225, 15.3113])\n",
            "tensor([65.8085, 69.5891, 48.3225, 15.3113])\n",
            "tensor([65.8085, 69.5891, 48.3225, 15.3113])\n",
            "tensor([65.8085, 69.5891, 48.3225, 15.3113])\n",
            "tensor([65.8085, 69.5891, 48.3225, 15.3113])\n",
            "tensor([65.8085, 69.5891, 48.3225, 15.3113])\n",
            "tensor([65.8085, 69.5891, 48.3225, 15.3113])\n",
            "tensor([65.8085, 69.5891, 48.3225, 15.3113])\n",
            "tensor([65.8085, 69.5891, 48.3225, 15.3113])\n",
            "tensor([65.8085, 69.5891, 48.3225, 15.3113])\n",
            "tensor([65.8085, 69.5891, 48.3225, 15.3113])\n",
            "tensor([65.8085, 69.5891, 48.3225, 15.3113])\n",
            "tensor([65.8085, 69.5891, 48.3225, 15.3113])\n",
            "tensor([65.8085, 69.5891, 48.3225, 15.3113])\n",
            "tensor([65.8085, 69.5891, 48.3225, 15.3113])\n",
            "tensor([65.8085, 69.5891, 48.3225, 15.3113])\n",
            "tensor([65.8085, 69.5891, 48.3225, 15.3113])\n",
            "tensor([65.8085, 69.5891, 48.3225, 15.3113])\n",
            "tensor([65.8085, 69.5891, 48.3225, 15.3113])\n",
            "tensor([65.8085, 69.5891, 48.3225, 15.3113])\n",
            "tensor([65.8085, 69.5891, 48.3225, 15.3113])\n",
            "tensor([65.8085, 69.5891, 48.3225, 15.3113])\n",
            "tensor([65.8085, 69.5891, 48.3225, 15.3113])\n",
            "tensor([65.8085, 69.5891, 48.3225, 15.3113])\n",
            "tensor([65.8085, 69.5891, 48.3225, 15.3113])\n",
            "tensor([65.8085, 69.5891, 48.3225, 15.3113])\n",
            "tensor([65.8085, 69.5891, 48.3225, 15.3113])\n",
            "tensor([65.8085, 69.5891, 48.3225, 15.3113])\n",
            "tensor([65.8085, 69.5891, 48.3225, 15.3113])\n",
            "tensor([65.8085, 69.5891, 48.3225, 15.3113])\n",
            "tensor([65.8085, 69.5891, 48.3225, 15.3113])\n",
            "tensor([65.8085, 69.5891, 48.3225, 15.3113])\n",
            "tensor([65.8085, 69.5891, 48.3225, 15.3113])\n",
            "tensor([65.8085, 69.5891, 48.3225, 15.3113])\n",
            "tensor([65.8085, 69.5891, 48.3225, 15.3113])\n",
            "tensor([65.8085, 69.5891, 48.3225, 15.3113])\n",
            "tensor([65.8085, 69.5891, 48.3225, 15.3113])\n",
            "tensor([65.8085, 69.5891, 48.3225, 15.3113])\n",
            "tensor([65.8085, 69.5891, 48.3225, 15.3113])\n",
            "tensor([65.8085, 69.5891, 48.3225, 15.3113])\n",
            "tensor([65.8085, 69.5891, 48.3225, 15.3113])\n",
            "tensor([65.8085, 69.5891, 48.3225, 15.3113])\n",
            "tensor([65.8085, 69.5891, 48.3225, 15.3113])\n",
            "tensor([65.8085, 69.5891, 48.3225, 15.3113])\n",
            "tensor([65.8085, 69.5891, 48.3225, 15.3113])\n",
            "tensor([65.8085, 69.5891, 48.3225, 15.3113])\n",
            "tensor([65.8085, 69.5891, 48.3225, 15.3113])\n",
            "tensor([65.8085, 69.5891, 48.3225, 15.3113])\n",
            "tensor([65.8085, 69.5891, 48.3225, 15.3113])\n",
            "tensor([65.8085, 69.5891, 48.3225, 15.3113])\n",
            "tensor([65.8085, 69.5891, 48.3225, 15.3113])\n",
            "tensor([65.8085, 69.5891, 48.3225, 15.3113])\n"
          ]
        }
      ],
      "source": [
        "losses = []\n",
        "for epoch in range(100):\n",
        "  print(neuron.w)\n",
        "  for x_example, y_example in zip(X, y): #zip(X, y) - объединяет два объекта X и y попарно\n",
        "    y_pred = neuron.forward(x_example)\n",
        "    curr_loss = loss.forward(y_pred,y_example)\n",
        "    losses.append(curr_loss)\n",
        "\n",
        "    loss.backward(y_pred,y_example)\n",
        "    neuron.backward(loss.dinput) #dinput-градиент функции потерь\n",
        "\n",
        "    neuron.w-=lr*neuron.dweights\n",
        "    neuron.b-=lr*neuron.dbias"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "coef"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kji0NokjR-Li",
        "outputId": "7680bb0e-87a6-4842-f794-e773442fbdc8"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([65.80849942, 69.58911489, 48.32251677, 15.31132061])"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "neuron.w"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tyyfKa3uSBBh",
        "outputId": "7a5b968b-9ef2-4b31-d110-cb3bf8db6dbc"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([65.8085, 69.5891, 48.3225, 15.3113])"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dr9qq4H_J3zt"
      },
      "source": [
        "2.4.1.1 Модифицируйте класс `MSELoss` из __2.3.1__, реализовав расчет производной относительно предыдущего слоя с учетом того, что теперь работа ведется с батчами, а не с индивидуальными примерами\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "L8wjk9iPMQ4x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d543302f-f7a9-4cc4-d2cf-4b85f931a53b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Значение MSE Loss: tensor(32202.0098)\n",
            "Градиент df/dy^: tensor([[-120.0768, -119.4102, -119.4102],\n",
            "        [-119.4102, -120.0768, -119.4102],\n",
            "        [-119.4102, -119.4102, -120.0768]])\n"
          ]
        }
      ],
      "source": [
        "class MSELoss:\n",
        "    def forward(self, y_pred, y_true):\n",
        "        # Рассчитываем MSE\n",
        "        self.diff = y_pred - y_true\n",
        "        mse = torch.mean(self.diff ** 2) #torch.mean - среднее значение всех элементов в тензоре\n",
        "        return mse\n",
        "\n",
        "    def backward(self):\n",
        "        # Рассчитываем производную относительно y_pred (df/dy^)\n",
        "        num_samples = self.diff.size(0)\n",
        "        self.dinput = 2 * self.diff / num_samples  # Усредняем по батчу (ев запомни уже, что батча это группа обучающих примеров,которые обрабатываются одновременно моделью.)\n",
        "\n",
        "mse_loss = MSELoss()\n",
        "\n",
        "loss = mse_loss.forward(y_pred, y_true)\n",
        "print(\"Значение MSE Loss:\", loss)\n",
        "\n",
        "mse_loss.backward()\n",
        "print(\"Градиент df/dy^:\",mse_loss.dinput)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3fSHCEtJjX8"
      },
      "source": [
        "2.4.2.2. Модифицируйте класс `Neuron` из __2.4.1.2__:\n",
        "\n",
        "  1) Реализуйте метод `forward` таким образом, чтобы он мог принимать на вход матрицу (батч) с данными.\n",
        "\n",
        "  2) Реализуйте расчет градиента относительно весов `weights` и `bias` с учетом того, что теперь работа ведется с батчами, а не с индивидуальными примерами"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "o_OpuAP0Jpz1"
      },
      "outputs": [],
      "source": [
        "class Neuron:\n",
        "    def __init__(self, n_inputs):\n",
        "        # Инициализируем веса и смещение случайными значениями\n",
        "        self.weights = torch.randn(n_inputs)\n",
        "        self.bias = torch.randn(1)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # Производим прямой проход через нейрон для батча\n",
        "        self.inputs = inputs\n",
        "        return torch.matmul(inputs, self.weights) + self.bias\n",
        "\n",
        "    def backward(self, dvalue):\n",
        "        # Рассчитываем градиенты для весов и смещения с учетом батча\n",
        "        self.dweights = torch.matmul(self.inputs.T, dvalue)  # df/dW\n",
        "        self.dbias = torch.sum(dvalue, dim=0, keepdim=True)  # df/db\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_inputs = 4\n",
        "neuron = Neuron(n_inputs)\n",
        "\n",
        "batch_size = 3\n",
        "inputs = torch.tensor([[1.0, 2.0, 3.0, 2.5],\n",
        "                       [2.0, 5.0, -1.0, 2.0],\n",
        "                       [-1.5, 2.7, 3.3, -0.8]])\n",
        "\n",
        "# Производим прямой проход через нейрон для батча данных\n",
        "outputs = neuron.forward(inputs)\n",
        "\n",
        "print(\"Выходы нейрона для батча данных:\")\n",
        "print(outputs)\n",
        "\n",
        "# Рассчитываем градиенты для батча\n",
        "dvalue = torch.randn(batch_size, 1)\n",
        "# Производим обратный проход и расчет градиентов для батча\n",
        "neuron.backward(dvalue)\n",
        "\n",
        "\n",
        "print(\"Градиенты для весов (dweights):\")\n",
        "print(neuron.dweights)\n",
        "print(\"Градиент для смещения (dbias):\")\n",
        "print(neuron.dbias)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DoBsc14_hnnr",
        "outputId": "52e69b35-679c-4ebb-b986-7a7da53537d4"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Выходы нейрона для батча данных:\n",
            "tensor([-8.5923, -3.5978, -5.8612])\n",
            "Градиенты для весов (dweights):\n",
            "tensor([[ 0.5954],\n",
            "        [-8.3179],\n",
            "        [-7.7524],\n",
            "        [-1.9727]])\n",
            "Градиент для смещения (dbias):\n",
            "tensor([[-2.9780]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zO-NZrgKMBFx"
      },
      "source": [
        "2.4.2.3 Допишите цикл для настройки весов нейрона"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "Zqwm_7eqJim1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d7783ba-9826-4393-a691-18f11f8dbec1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Эпоха 0: MSE Loss = 11.987822532653809\n",
            "Эпоха 10: MSE Loss = 11.987822532653809\n",
            "Эпоха 20: MSE Loss = 11.987822532653809\n",
            "Эпоха 30: MSE Loss = 11.987822532653809\n",
            "Эпоха 40: MSE Loss = 11.987822532653809\n",
            "Эпоха 50: MSE Loss = 11.987822532653809\n",
            "Эпоха 60: MSE Loss = 11.987822532653809\n",
            "Эпоха 70: MSE Loss = 11.987822532653809\n",
            "Эпоха 80: MSE Loss = 11.987822532653809\n",
            "Эпоха 90: MSE Loss = 11.987822532653809\n",
            "Окончательные веса нейрона: tensor([-0.1883, -1.0610,  0.5478,  0.4748])\n",
            "Окончательное смещение нейрона: tensor([-0.1088])\n"
          ]
        }
      ],
      "source": [
        "class Neuron:\n",
        "    def __init__(self, n_inputs):\n",
        "        self.weights = torch.randn(n_inputs)\n",
        "        self.bias = torch.randn(1)\n",
        "        self.dweights = torch.zeros_like(self.weights)\n",
        "        self.dbias = torch.zeros_like(self.bias)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        self.inputs = inputs\n",
        "        return torch.matmul(inputs, self.weights) + self.bias\n",
        "\n",
        "    def backward(self, dvalue):\n",
        "        self.dweights = torch.matmul(self.inputs.T, dvalue)  # df/dW\n",
        "        self.dbias = torch.sum(dvalue, dim=0, keepdim=True)  # df/db\n",
        "\n",
        "\n",
        "n_inputs = inputs.size(1)  # Размерность элемента выборки\n",
        "learning_rate = 0.1  # Скорость обучения\n",
        "n_epoch = 100  # Количество эпох\n",
        "\n",
        "inputs = torch.tensor([[1.0, 2.0, 3.0, 2.5],\n",
        "                       [2.0, 5.0, -1.0, 2.0],\n",
        "                       [-1.5, 2.7, 3.3, -0.8]])\n",
        "\n",
        "neuron = Neuron(n_inputs)\n",
        "loss = MSELoss()\n",
        "losses = []\n",
        "\n",
        "for epoch in range(n_epoch):\n",
        "    # Forward pass\n",
        "    y_pred = neuron.forward(inputs)\n",
        "    curr_loss = loss.forward(y_pred, y_true)\n",
        "    losses.append(curr_loss.item())  # Сохраняем текущее значение потери в списке\n",
        "\n",
        "    # Backpropagation\n",
        "    loss.backward()  # Рассчитываем градиенты относительно y_pred\n",
        "\n",
        "    # Обновляем веса нейрона с учетом градиентов и скорости обучения\n",
        "    neuron.weights -= learning_rate * neuron.dweights\n",
        "    neuron.bias -= learning_rate * neuron.dbias\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "        print(f\"Эпоха {epoch}: MSE Loss = {curr_loss.item()}\")\n",
        "\n",
        "print(\"Окончательные веса нейрона:\", neuron.weights)\n",
        "print(\"Окончательное смещение нейрона:\", neuron.bias)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16VtP159OdMk"
      },
      "source": [
        "2.4.3  Используя один полносвязный слой и  пакетный градиетный спуск, решите задачу регрессии из __2.4.1__"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "X, y, coef = make_regression(n_features=4, n_informative=4, coef=True, bias=0.5)\n",
        "X = torch.from_numpy(X).to(torch.float32)\n",
        "y = torch.from_numpy(y).to(torch.float32)\n",
        "\n",
        "# Разделяем данные на обучающий и тестовый наборы\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Определяем архитектуру нейронной сети\n",
        "class RegressionModel(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super(RegressionModel, self).__init__()\n",
        "        self.fc = nn.Linear(input_size, 1)  # создает полносвязный слой, который принимает на вход 4 признака(n_features=4, n_informative=4, coef=True, bias=0.5) и имеет один выходной нейрон\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)\n",
        "\n",
        "# Создаем модель и определяем функцию потерь и оптимизатор\n",
        "input_size = X_train.shape[1]  # Размерность входных данных\n",
        "model = RegressionModel(input_size)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)  # Оптимизатор SGD с заданной скоростью обучения\n",
        "\n",
        "# Обучаем модель с использованием пакетного градиентного спуска\n",
        "num_epochs = 100\n",
        "batch_size = 32\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # Разбиваем данные на пакеты\n",
        "    for i in range(0, len(X_train), batch_size):\n",
        "        batch_X = X_train[i:i+batch_size]\n",
        "        batch_y = y_train[i:i+batch_size]\n",
        "\n",
        "        # Обнуляем градиенты\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Прямой проход\n",
        "        outputs = model(batch_X)\n",
        "\n",
        "        # Рассчитываем функцию потерь\n",
        "        loss = criterion(outputs, batch_y)\n",
        "\n",
        "        # Обратный проход и оптимизация\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Выводим значение функции потерь на каждой эпохе\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}')\n",
        "\n",
        "# Оцениваем модель на тестовом наборе данных\n",
        "with torch.no_grad():\n",
        "    test_outputs = model(X_test)\n",
        "    test_loss = criterion(test_outputs, y_test)\n",
        "    print(f'Loss on test data: {test_loss.item()}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VmZ5VPnOk3Ag",
        "outputId": "51b59af3-2297-41c0-921c-31600c71e392"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([16])) that is different to the input size (torch.Size([16, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/100], Loss: 16110.01953125\n",
            "Epoch [2/100], Loss: 16113.03125\n",
            "Epoch [3/100], Loss: 16115.9970703125\n",
            "Epoch [4/100], Loss: 16118.8984375\n",
            "Epoch [5/100], Loss: 16121.720703125\n",
            "Epoch [6/100], Loss: 16124.4541015625\n",
            "Epoch [7/100], Loss: 16127.095703125\n",
            "Epoch [8/100], Loss: 16129.6337890625\n",
            "Epoch [9/100], Loss: 16132.0703125\n",
            "Epoch [10/100], Loss: 16134.404296875\n",
            "Epoch [11/100], Loss: 16136.630859375\n",
            "Epoch [12/100], Loss: 16138.75390625\n",
            "Epoch [13/100], Loss: 16140.771484375\n",
            "Epoch [14/100], Loss: 16142.6923828125\n",
            "Epoch [15/100], Loss: 16144.5126953125\n",
            "Epoch [16/100], Loss: 16146.23828125\n",
            "Epoch [17/100], Loss: 16147.8720703125\n",
            "Epoch [18/100], Loss: 16149.41796875\n",
            "Epoch [19/100], Loss: 16150.8779296875\n",
            "Epoch [20/100], Loss: 16152.259765625\n",
            "Epoch [21/100], Loss: 16153.5615234375\n",
            "Epoch [22/100], Loss: 16154.7900390625\n",
            "Epoch [23/100], Loss: 16155.9482421875\n",
            "Epoch [24/100], Loss: 16157.0390625\n",
            "Epoch [25/100], Loss: 16158.0673828125\n",
            "Epoch [26/100], Loss: 16159.037109375\n",
            "Epoch [27/100], Loss: 16159.9482421875\n",
            "Epoch [28/100], Loss: 16160.8056640625\n",
            "Epoch [29/100], Loss: 16161.6142578125\n",
            "Epoch [30/100], Loss: 16162.3720703125\n",
            "Epoch [31/100], Loss: 16163.087890625\n",
            "Epoch [32/100], Loss: 16163.759765625\n",
            "Epoch [33/100], Loss: 16164.3916015625\n",
            "Epoch [34/100], Loss: 16164.986328125\n",
            "Epoch [35/100], Loss: 16165.544921875\n",
            "Epoch [36/100], Loss: 16166.0703125\n",
            "Epoch [37/100], Loss: 16166.5634765625\n",
            "Epoch [38/100], Loss: 16167.0283203125\n",
            "Epoch [39/100], Loss: 16167.46484375\n",
            "Epoch [40/100], Loss: 16167.8759765625\n",
            "Epoch [41/100], Loss: 16168.259765625\n",
            "Epoch [42/100], Loss: 16168.623046875\n",
            "Epoch [43/100], Loss: 16168.96484375\n",
            "Epoch [44/100], Loss: 16169.2861328125\n",
            "Epoch [45/100], Loss: 16169.5859375\n",
            "Epoch [46/100], Loss: 16169.869140625\n",
            "Epoch [47/100], Loss: 16170.1376953125\n",
            "Epoch [48/100], Loss: 16170.38671875\n",
            "Epoch [49/100], Loss: 16170.6240234375\n",
            "Epoch [50/100], Loss: 16170.8447265625\n",
            "Epoch [51/100], Loss: 16171.0546875\n",
            "Epoch [52/100], Loss: 16171.251953125\n",
            "Epoch [53/100], Loss: 16171.435546875\n",
            "Epoch [54/100], Loss: 16171.611328125\n",
            "Epoch [55/100], Loss: 16171.7744140625\n",
            "Epoch [56/100], Loss: 16171.9287109375\n",
            "Epoch [57/100], Loss: 16172.07421875\n",
            "Epoch [58/100], Loss: 16172.2119140625\n",
            "Epoch [59/100], Loss: 16172.3388671875\n",
            "Epoch [60/100], Loss: 16172.4609375\n",
            "Epoch [61/100], Loss: 16172.576171875\n",
            "Epoch [62/100], Loss: 16172.68359375\n",
            "Epoch [63/100], Loss: 16172.78515625\n",
            "Epoch [64/100], Loss: 16172.880859375\n",
            "Epoch [65/100], Loss: 16172.970703125\n",
            "Epoch [66/100], Loss: 16173.0556640625\n",
            "Epoch [67/100], Loss: 16173.1357421875\n",
            "Epoch [68/100], Loss: 16173.2109375\n",
            "Epoch [69/100], Loss: 16173.283203125\n",
            "Epoch [70/100], Loss: 16173.3515625\n",
            "Epoch [71/100], Loss: 16173.412109375\n",
            "Epoch [72/100], Loss: 16173.47265625\n",
            "Epoch [73/100], Loss: 16173.529296875\n",
            "Epoch [74/100], Loss: 16173.583984375\n",
            "Epoch [75/100], Loss: 16173.634765625\n",
            "Epoch [76/100], Loss: 16173.681640625\n",
            "Epoch [77/100], Loss: 16173.7265625\n",
            "Epoch [78/100], Loss: 16173.7685546875\n",
            "Epoch [79/100], Loss: 16173.80859375\n",
            "Epoch [80/100], Loss: 16173.8466796875\n",
            "Epoch [81/100], Loss: 16173.8828125\n",
            "Epoch [82/100], Loss: 16173.9169921875\n",
            "Epoch [83/100], Loss: 16173.9482421875\n",
            "Epoch [84/100], Loss: 16173.978515625\n",
            "Epoch [85/100], Loss: 16174.0078125\n",
            "Epoch [86/100], Loss: 16174.03515625\n",
            "Epoch [87/100], Loss: 16174.060546875\n",
            "Epoch [88/100], Loss: 16174.083984375\n",
            "Epoch [89/100], Loss: 16174.1083984375\n",
            "Epoch [90/100], Loss: 16174.12890625\n",
            "Epoch [91/100], Loss: 16174.150390625\n",
            "Epoch [92/100], Loss: 16174.1689453125\n",
            "Epoch [93/100], Loss: 16174.1875\n",
            "Epoch [94/100], Loss: 16174.205078125\n",
            "Epoch [95/100], Loss: 16174.2197265625\n",
            "Epoch [96/100], Loss: 16174.2373046875\n",
            "Epoch [97/100], Loss: 16174.251953125\n",
            "Epoch [98/100], Loss: 16174.2666015625\n",
            "Epoch [99/100], Loss: 16174.27734375\n",
            "Epoch [100/100], Loss: 16174.2900390625\n",
            "Loss on test data: 17768.9453125\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([20])) that is different to the input size (torch.Size([20, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uj5febreSSZ7"
      },
      "source": [
        "2.4.3.1 Модифицируйте класс `Linear` из __2.1.4__. ([вычисление градиентов](https://i.ibb.co/kgVR6m6/photo-2021-02-15-21-30-28.jpg))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9zWuhaLdSB2_"
      },
      "outputs": [],
      "source": [
        "class Linear:\n",
        "    def __init__(self, n_features, n_neurons):\n",
        "        self.weights = torch.randn(n_features, n_neurons)\n",
        "        self.biases = torch.zeros(1, n_neurons)  # Создание тензора для смещений\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        self.inputs = inputs\n",
        "        self.output = torch.matmul(inputs, self.weights) + self.biases\n",
        "        return self.output\n",
        "\n",
        "    def backward(self, dvalues):\n",
        "        # Градиент по весам, смещениям и входам\n",
        "        self.dweights = torch.matmul(self.inputs.T, dvalues)\n",
        "        self.dbiases = torch.sum(dvalues, axis=0, keepdim=True)\n",
        "        self.dinputs = torch.matmul(dvalues, self.weights.T)\n",
        "        return self.dinputs, self.dweights, self.dbiases\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTkJV-F8TVuN"
      },
      "source": [
        "2.4.4 Используя наработки из 2.4, создайте нейросеть и решите задачу регрессии.\n",
        "\n",
        "Предлагаемая архитектура:\n",
        "1. Полносвязный слой с 10 нейронами\n",
        "2. Активация ReLU\n",
        "3. Полносвязный слой с 1 нейроном"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "axUjpPz-SvS1"
      },
      "outputs": [],
      "source": [
        "X = torch.linspace(-1, 1, 100).view(-1, 1)\n",
        "y = X.pow(2) + 0.2 * torch.rand(X.size())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LXoiNxkpTziV"
      },
      "outputs": [],
      "source": [
        "class Activation_ReLU:\n",
        "  def forward(self, inputs):\n",
        "    self.inputs = inputs\n",
        "    self.output = inputs.clip(min=0)\n",
        "    return self.output\n",
        "\n",
        "  def backward(self, dvalues):\n",
        "    self.dinputs = dvalues.clone()\n",
        "    self.dinputs[self.inputs <= 0] = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tXhspwW6T44T"
      },
      "outputs": [],
      "source": [
        "# создание компонентов сети\n",
        "# fc1 =\n",
        "# relu1 =\n",
        "# fc2 =\n",
        "\n",
        "loss = MSELoss()\n",
        "lr = 0.02\n",
        "\n",
        "ys = []\n",
        "for epoch in range(2001):\n",
        "  # <forward pass>\n",
        "  # fc1 > relu1 > fc2 > loss\n",
        "\n",
        "  data_loss = # <прогон через функцию потерь>\n",
        "\n",
        "  if epoch % 200 == 0:\n",
        "    print(f'epoch {epoch} mean loss {data_loss}')\n",
        "    ys.append(out)\n",
        "\n",
        "  # <backprop>\n",
        "  # loss > fc2 > relu1 > fc1\n",
        "\n",
        "  # <шаг оптимизации для fc1>\n",
        "\n",
        "  # <шаг оптимизации для fc2>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kpKi0OfoUkwk"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, axs = plt.subplots(len(ys), 1, figsize=(10, 40))\n",
        "for ax, y_ in zip(axs, ys):\n",
        "  ax.scatter(X.numpy(), y.numpy(), color = \"orange\")\n",
        "  ax.plot(X.numpy(), y_.numpy(), 'g-', lw=3)\n",
        "  ax.set_xlim(-1.05, 1.5)\n",
        "  ax.set_ylim(-0.25, 1.25)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}