{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ygarkakoito/neuron/blob/main/blank__05_egorova_eva.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XtFQP3RNll3c"
      },
      "source": [
        "import re\n",
        "import nltk\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.preprocessing import LabelEncoder"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "443CZCLp0_sE",
        "outputId": "c08f3172-2b99-4f9a-ba2b-6168deb18aff"
      },
      "source": [
        "nltk.download('punkt')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jqDHq_AEjRZ1"
      },
      "source": [
        "## 1. Представление и предобработка текстовых данных"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vaki7efDpmXo"
      },
      "source": [
        "1.1 Операции по предобработке:\n",
        "* токенизация\n",
        "* стемминг / лемматизация\n",
        "* удаление стоп-слов\n",
        "* удаление пунктуации\n",
        "* приведение к нижнему регистру\n",
        "* любые другие операции над текстом"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nHRy4jpYphEr"
      },
      "source": [
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.stem.snowball import SnowballStemmer"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMMzGhq0ikz1"
      },
      "source": [
        "text = 'Select your preferences and run the install command. Stable represents the most currently tested and supported version of PyTorch. Note that LibTorch is only available for C++'"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bUhfertRtXE5"
      },
      "source": [
        "Реализовать функцию `preprocess_text(text: str)`, которая:\n",
        "* приводит строку к нижнему регистру\n",
        "* заменяет все символы, кроме a-z, A-Z и знаков .,!? на пробел\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "import re\n",
        "\n",
        "def preprocess_text(text: str) -> str:\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^a-zA-Z.,!?]', ' ', text)\n",
        "    return text\n",
        "\n",
        "text = 'Select your preferences and run the install command. Stable represents the most currently tested and supported version of PyTorch. Note that LibTorch is only available for C++'\n",
        "processed_text = preprocess_text(text)\n",
        "print(processed_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LlNp4eH9mRcR",
        "outputId": "e8502d5d-b8ab-4d44-e8d1-161220dd3671"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "select your preferences and run the install command. stable represents the most currently tested and supported version of pytorch. note that libtorch is only available for c  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z2Dt1ssIqckC"
      },
      "source": [
        "1.2 Представление текстовых данных при помощи бинарного кодирования\n",
        "\n",
        "\n",
        "Представить первое предложение из `text` в виде тензора `sentence_t`: `sentence_t[i] == 1`, если __слово__ с индексом `i` присуствует в предложении."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from nltk.tokenize import word_tokenize\n",
        "text = 'Select your preferences and run the install command. Stable represents the most currently tested and supported version of PyTorch. Note that LibTorch is only available for C++'\n",
        "tokens = word_tokenize(text)\n",
        "unique_words = sorted(set(tokens))\n",
        "sentence_t = torch.zeros(len(unique_words))\n",
        "for i, word in enumerate(unique_words):\n",
        "    if word in tokens:\n",
        "        sentence_t[i] = 1\n",
        "print(\"Тензор предложения:\",sentence_t)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1--m0xb0mujM",
        "outputId": "69b4cbcd-18cf-4887-e39d-debac4ad0b58"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Тензор предложения: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2Nz_zcgw3N4"
      },
      "source": [
        "## 2. Классификация фамилий по национальности\n",
        "\n",
        "Датасет: https://disk.yandex.ru/d/owHew8hzPc7X9Q?w=1\n",
        "\n",
        "2.1 Считать файл `surnames/surnames.csv`.\n",
        "\n",
        "2.2 Закодировать национальности числами, начиная с 0.\n",
        "\n",
        "2.3 Разбить датасет на обучающую и тестовую выборку\n",
        "\n",
        "2.4 Реализовать класс `Vocab` (токен = __символ__)\n",
        "\n",
        "2.5 Реализовать класс `SurnamesDataset`\n",
        "\n",
        "2.6. Обучить классификатор.\n",
        "\n",
        "2.7 Измерить точность на тестовой выборке. Проверить работоспособность модели: прогнать несколько фамилий студентов группы через модели и проверить результат. Для каждой фамилии выводить 3 наиболее вероятных предсказания."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch\n",
        "from torch.nn.utils.rnn import pad_sequence\n"
      ],
      "metadata": {
        "id": "Q_Fi9Mg0oDP4"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Шаг 2.1: Считываем файл\n",
        "file_path = 'surnames/surnames.csv'\n",
        "df = pd.read_csv(file_path)"
      ],
      "metadata": {
        "id": "Sl6HiZUKoEtZ"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Шаг 2.2: Кодируем национальности числами\n",
        "nationalities = df['nationality'].unique()\n",
        "nationality_to_idx = {nat: idx for idx, nat in enumerate(nationalities)}\n",
        "idx_to_nationality = {idx: nat for idx, nat in enumerate(nationalities)}\n",
        "df['nationality'] = df['nationality'].map(nationality_to_idx)"
      ],
      "metadata": {
        "id": "AsGU1AqSoHkc"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Шаг 2.3: Разбиваем датасет на обучающую и тестовую выборку\n",
        "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "V2QNXA40oNFU"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUkSZkDqxNYS"
      },
      "source": [
        "# Шаг 2.4: Реализуем класс Vocab\n",
        "class Vocab:\n",
        "    def __init__(self, data):\n",
        "        self.idx_to_token = {i: char for i, char in enumerate(data)}\n",
        "        self.token_to_idx = {char: i for i, char in enumerate(data)}\n",
        "        self.vocab_len = len(data)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WCaRK1QHxe0A"
      },
      "source": [
        "class SurnamesDataset(Dataset):\n",
        "    def __init__(self, X, y, vocab: Vocab):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        self.vocab = vocab\n",
        "\n",
        "    def vectorize(self, surname):\n",
        "        '''Генерирует представление фамилии surname в при помощи бинарного кодирования'''\n",
        "        vectorized_surname = [self.vocab.token_to_idx[char] for char in surname]\n",
        "        return torch.tensor(vectorized_surname)\n",
        "\n",
        "    def collate_fn(self, batch):\n",
        "            surnames = [item['surname'] for item in batch]\n",
        "            nationalities = [item['nationality'] for item in batch]\n",
        "\n",
        "            # Добавляем пробел в словарь, если его там нет\n",
        "            if ' ' not in self.vocab.token_to_idx:\n",
        "                self.vocab.token_to_idx[' '] = len(self.vocab.token_to_idx)\n",
        "                self.vocab.idx_to_token[len(self.vocab.token_to_idx) - 1] = ' '\n",
        "\n",
        "            # Заполняем паддингами до максимальной длины в пакете\n",
        "            padded_surnames = pad_sequence(surnames, batch_first=True, padding_value=self.vocab.token_to_idx[' '])\n",
        "\n",
        "            return {'surname': padded_surnames, 'nationality': torch.tensor(nationalities)}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        surname = self.X.iloc[idx]\n",
        "        nationality = self.y.iloc[idx]\n",
        "        vectorized_surname = self.vectorize(surname)\n",
        "        return {'surname': vectorized_surname, 'nationality': nationality}"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Шаг 2.6: Обучение модели\n",
        "class SimpleClassifier(nn.Module):\n",
        "    def __init__(self, input_size, output_size):\n",
        "        super(SimpleClassifier, self).__init__()\n",
        "        self.embedding = nn.Embedding(input_size, 50)\n",
        "        self.fc = nn.Linear(50, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x = torch.mean(x, dim=1)\n",
        "        x = self.fc(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "e5kYtrGIoUh5"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Подготовка данных\n",
        "input_size = len(Vocab(\"\".join(df['surname'].values)).idx_to_token)\n",
        "output_size = len(nationalities)\n",
        "\n",
        "train_dataset = SurnamesDataset(train_df['surname'], train_df['nationality'], Vocab(\"\".join(df['surname'].values)))\n",
        "test_dataset = SurnamesDataset(test_df['surname'], test_df['nationality'], Vocab(\"\".join(df['surname'].values)))"
      ],
      "metadata": {
        "id": "KSqLBJZNoXYq"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Используем collate_fn для обработки пакетов с переменной длиной\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, collate_fn=train_dataset.collate_fn)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, collate_fn=test_dataset.collate_fn)\n"
      ],
      "metadata": {
        "id": "jfZ3KvjLoZc8"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Обучение модели\n",
        "model = SimpleClassifier(input_size, output_size)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    for batch in train_loader:\n",
        "        inputs, targets = batch['surname'], batch['nationality']\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()"
      ],
      "metadata": {
        "id": "tDwsKdB8oss7"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Шаг 2.7: Измерение точности на тестовой выборке\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        inputs, targets = batch['surname'], batch['nationality']\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += targets.size(0)\n",
        "        correct += (predicted == targets).sum().item()\n",
        "\n",
        "accuracy = correct / total\n",
        "print(f'Test Accuracy: {accuracy * 100:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DqXxjCUppP4u",
        "outputId": "456310b8-a3ef-4720-f085-8879f7d0b7cb"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 51.87%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PLmDB3fJtVox"
      },
      "source": [
        "## 3. Классификация обзоров ресторанов\n",
        "\n",
        "Датасет: https://disk.yandex.ru/d/nY1o70JtAuYa8g\n",
        "\n",
        "3.1 Считать файл `yelp/raw_train.csv`. Оставить от исходного датасета 10% строчек.\n",
        "\n",
        "3.2 Воспользоваться функцией `preprocess_text` из 1.1 для обработки текста отзыва. Закодировать рейтинг числами, начиная с 0.\n",
        "\n",
        "3.3 Разбить датасет на обучающую и тестовую выборку\n",
        "\n",
        "3.4 Реализовать класс `Vocab` (токен = слово)\n",
        "\n",
        "3.5 Реализовать класс `ReviewDataset`\n",
        "\n",
        "3.6 Обучить классификатор\n",
        "\n",
        "3.7 Измерить точность на тестовой выборке. Проверить работоспособность модели: придумать небольшой отзыв, прогнать его через модель и вывести номер предсказанного класса (сделать это для явно позитивного и явно негативного отзыва)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vuuDvmj7n50l"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lknAigmxn_g2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_lCTSKZgu68K"
      },
      "source": [
        "class Vocab:\n",
        "  def __init__(self, data):\n",
        "    self.idx_to_token = ...\n",
        "    self.token_to_idx = ...\n",
        "    self.vocab_len = ..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXLmCDvcvRmb"
      },
      "source": [
        "class ReviewDataset(Dataset):\n",
        "  def __init__(self, X, y, vocab: Vocab):\n",
        "    self.X = X\n",
        "    self.y = y\n",
        "    self.vocab = vocab\n",
        "\n",
        "  def vectorize(self, review):\n",
        "    '''Генерирует представление отзыва review при помощи бинарного кодирования (см. 1.2)'''\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.X)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return ..."
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}